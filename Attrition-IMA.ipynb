{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a97f9a4d",
   "metadata": {},
   "source": [
    "# Attrition Prediction\n",
    "### By Sejal, Ethan, Srivignesh, Nurzhan, Sigit, Mihier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6668c637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import plotly.figure_factory as ff\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection  import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6b74f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV\n",
    "employee_df = pd.read_csv('/Users/sri/Downloads/IMA_2.csv')\n",
    "\n",
    "# Count of missing values for each column in the dataset\n",
    "employee_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8f2bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create bar graphs\n",
    "def barplot(var_select, x_no_numeric) :\n",
    "    # Two variables for Attrition = 'Yes' and 'No'\n",
    "    tmp1 = employee_df[(employee_df['Attrition'] == 'Yes')]\n",
    "    tmp2 = employee_df[(employee_df['Attrition'] == 'No')]\n",
    "    # Compute cross tabulation between variable and Attrition\n",
    "    tmp3 = pd.DataFrame(pd.crosstab(employee_df[var_select],employee_df['Attrition']), )\n",
    "    # Calculate attrition % based on the selected variable\n",
    "    tmp3['Attr%'] = tmp3['Yes'] / (tmp3['Yes'] + tmp3['No']) * 100\n",
    "    if x_no_numeric == True  : \n",
    "        tmp3 = tmp3.sort_values('Yes', ascending = False)\n",
    "\n",
    "    # Plot bar for count of variable with attrition\n",
    "    trace1 = go.Bar(\n",
    "        x=tmp1[var_select].value_counts().keys().tolist(),\n",
    "        y=tmp1[var_select].value_counts().values.tolist(),\n",
    "        name='Yes_Attrition',opacity = 0.8, marker=dict(\n",
    "        color='gold',\n",
    "        line=dict(color='#000000',width=1)))\n",
    "\n",
    "    # Plot bar for count of variable without attrition\n",
    "    trace2 = go.Bar(\n",
    "        x=tmp2[var_select].value_counts().keys().tolist(),\n",
    "        y=tmp2[var_select].value_counts().values.tolist(),\n",
    "        name='No_Attrition', opacity = 0.8, marker=dict(\n",
    "        color='lightskyblue',\n",
    "        line=dict(color='#000000',width=1)))\n",
    "    \n",
    "    # Scatter plot of the attrition rate for each category\n",
    "    trace3 =  go.Scatter(   \n",
    "        x=tmp3.index,\n",
    "        y=tmp3['Attr%'],\n",
    "        yaxis = 'y2',\n",
    "        name='% Attrition', opacity = 0.6, marker=dict(\n",
    "        color='black',\n",
    "        line=dict(color='#000000',width=0.5\n",
    "        )))\n",
    "\n",
    "    layout = dict(title =  str(var_select),\n",
    "              xaxis=dict(), \n",
    "              yaxis=dict(title= 'Count'), \n",
    "              yaxis2=dict(range= [-0, 75], \n",
    "                          overlaying= 'y', \n",
    "                          anchor= 'x', \n",
    "                          side= 'right',\n",
    "                          zeroline=False,\n",
    "                          showgrid= False, \n",
    "                          title= '% Attrition'\n",
    "                         ))\n",
    "\n",
    "    fig = go.Figure(data=[trace1, trace2, trace3], layout=layout)\n",
    "    py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab35421a",
   "metadata": {},
   "source": [
    "## Data visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4164a0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Data visualisation\n",
    "# Count of attrition vs non-attrition\n",
    "trace1 = go.Bar(\n",
    "        x=employee_df['Attrition'].value_counts().keys().tolist(),\n",
    "        y=employee_df['Attrition'].value_counts().values.tolist(),\n",
    "        name='Attrition count',opacity = 0.8,\n",
    "        marker=dict(color=['lightskyblue','gold'])\n",
    "        )\n",
    "fig = go.Figure(data=[trace1])\n",
    "py.iplot(fig)\n",
    "\n",
    "# Plot bar graphs for all features vs attrition\n",
    "barplot('Age', False)\n",
    "barplot('Department',True)\n",
    "barplot('EducationField',True)\n",
    "barplot('Education', True)\n",
    "barplot('MaritalStatus',True)\n",
    "barplot('EnvironmentSatisfaction', True)\n",
    "barplot('JobSatisfaction', True)\n",
    "barplot('WorkLifeBalance', True)\n",
    "barplot('NumCompaniesWorked',False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6ceffc",
   "metadata": {},
   "source": [
    "# Data Cleaning & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64a4704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning\n",
    "# Add end date column to the dataframe\n",
    "# employee_df['EndDate'] = '27/01/2022'\n",
    "# Convert dates to datetime\n",
    "# employee_df['EndDate'] = pd.to_datetime(employee_df['EndDate'], errors='coerce', infer_datetime_format=True)\n",
    "employee_df['StartDate'] = pd.to_datetime(employee_df['StartDate'], errors='coerce', infer_datetime_format=True)\n",
    "\n",
    "# Calculate employment period\n",
    "#employee_df['EmploymentPeriod']=((employee_df.EndDate - employee_df.StartDate)/np.timedelta64(1, 'M'))\n",
    "#employee_df['EmploymentPeriod']=employee_df['EmploymentPeriod'].astype(int)\n",
    "\n",
    "## Convert attrition to numeric values\n",
    "employee_df['Attrition'] = np.where(employee_df.Attrition == 'Yes', 1, 0)\n",
    "\n",
    "## Remove useless columns\n",
    "employee_df = employee_df.drop(columns = ['StartDate','ID'], axis=1)\n",
    "\n",
    "## Convert distance to numeric\n",
    "employee_df['DistanceFromHome'] = employee_df['DistanceFromHome'].astype(str)\n",
    "employee_df['DistanceFromHome'] = employee_df['DistanceFromHome'].str.replace(' miles', '')\n",
    "employee_df['DistanceFromHome'] = employee_df['DistanceFromHome'].astype(int)\n",
    "\n",
    "## Convert Income to float\n",
    "employee_df['MonthlyIncome'] = employee_df['MonthlyIncome'].astype(str)\n",
    "employee_df['MonthlyIncome'] = employee_df['MonthlyIncome'].str.replace('Â£', '')\n",
    "employee_df['MonthlyIncome'] = employee_df['MonthlyIncome'].str.replace(',','')\n",
    "employee_df['MonthlyIncome'] = employee_df['MonthlyIncome'].astype(float)\n",
    "\n",
    "## Convert Scale values to numeric\n",
    "employee_df=employee_df.replace(to_replace=\"Very High\",value=\"3\")\n",
    "employee_df=employee_df.replace(to_replace=\"High\",value=\"2\")\n",
    "employee_df=employee_df.replace(to_replace=\"Medium\",value=\"1\")\n",
    "employee_df=employee_df.replace(to_replace=\"Low\",value=\"0\")\n",
    "\n",
    "employee_df=employee_df.replace(to_replace=\"Best\",value=\"3\")\n",
    "employee_df=employee_df.replace(to_replace= \"Better\",value=\"2\")\n",
    "employee_df=employee_df.replace(to_replace=\"Good\",value=\"1\")\n",
    "employee_df=employee_df.replace(to_replace=\"Bad\",value=\"0\")\n",
    "\n",
    "employee_df=employee_df.replace(to_replace=\"PhD\",value=\"4\")\n",
    "employee_df=employee_df.replace(to_replace=\"Masters\",value=\"3\")\n",
    "employee_df=employee_df.replace(to_replace=\"Bachelors\",value=\"2\")\n",
    "employee_df=employee_df.replace(to_replace=\"College\",value=\"1\")\n",
    "employee_df=employee_df.replace(to_replace=\"Below College\",value=\"0\")\n",
    "\n",
    "# Split categorical values into different columns using get_dummies\n",
    "employee_df = pd.get_dummies(employee_df, columns=[\"MaritalStatus\"], prefix=[\"TypeMaritalStatus\"])\n",
    "employee_df = pd.get_dummies(employee_df, columns=[\"Department\"], prefix=[\"TypeDepartment\"])\n",
    "employee_df = pd.get_dummies(employee_df, columns=[\"EducationField\"], prefix=[\"TypeEducationField\"])\n",
    "\n",
    "employee_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d0f850",
   "metadata": {},
   "source": [
    "### Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75addbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "fig = plt.figure(figsize=(15,25), dpi = 480)\n",
    "sns.heatmap(employee_df.corr(), annot = True, fmt = '.2f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97614e9a",
   "metadata": {},
   "source": [
    "### Prepare target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54e3bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a target variable\n",
    "model_target = employee_df['Attrition']\n",
    "target_dict = model_target.to_dict()\n",
    "\n",
    "# Drop Attrition\n",
    "employee_df = employee_df.drop(columns = ['Attrition'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73b2fb5",
   "metadata": {},
   "source": [
    "## Scale dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df56c1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale dataframe\n",
    "x = RobustScaler().fit_transform(employee_df)\n",
    "scaled_df = pd.DataFrame(x, columns=employee_df.columns)\n",
    "scaled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeabfeb1",
   "metadata": {},
   "source": [
    "#### Function to plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ebb26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalise=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues,\n",
    "                          multi=False):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalise=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalise:\n",
    "            title = 'Normalised confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalisation'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Only use the labels that appear in the data\n",
    "    if multi==True:\n",
    "    \tclasses = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalise:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\");\n",
    "\n",
    "    fmt = '.2f' if normalise else 'd'\n",
    "    thresh = cm.max() / 2\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ced13b",
   "metadata": {},
   "source": [
    "## Create Test-Train split and oversample train split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ed56f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train-test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(scaled_df, target_dict, test_size = 0.2)\n",
    "\n",
    "# Oversampling using SMOTE\n",
    "X_resampled, Y_resampled = SMOTE().fit_resample(X_train, Y_train)\n",
    "\n",
    "print(f'x_train: {X_train.shape}')\n",
    "print(f'x resampled: {X_resampled.shape}')\n",
    "\n",
    "print(f'Yes : {Y_resampled.count(1)}. No : {Y_resampled.count(0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e282ec0e",
   "metadata": {},
   "source": [
    "## Checking hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087203f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning hyperparameters for decision trees\n",
    "'''\n",
    "tuned_parameters = [{'criterion': ['gini', 'entropy'],\n",
    "                     'max_depth': [7, 9, 11, 13, 15,17, 19, 21],\n",
    "                     'min_samples_split': [3, 5, 7, 9],\n",
    "                     'max_features': [\"sqrt\", \"log2\", None]}]\n",
    "\n",
    "scores = ['accuracy', 'f1_macro', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyperparameters for %s\" % score)\n",
    "    print(\"\\n\")\n",
    "    clf = GridSearchCV(DTC(), tuned_parameters, cv=5,\n",
    "                       scoring= score)\n",
    "    clf.fit(X_resampled, Y_resampled)\n",
    "    print(\"Best parameters set found on the training set:\")\n",
    "    print(clf.best_params_)\n",
    "    print(\"\\n\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1672cee4",
   "metadata": {},
   "source": [
    "## Modeling and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627ed5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For decision tree\n",
    "#model = DTC(criterion='entropy', max_depth=19 , max_features=None, min_samples_split=3)\n",
    "\n",
    "# For logistic regression\n",
    "model = LogisticRegression()\n",
    "\n",
    "model_fit = model.fit(X_resampled, Y_resampled)\n",
    "\n",
    "print('-----------------')\n",
    "print(classification_report(Y_test, model_fit.predict(X_test)))\n",
    "print('-----------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d2ba68",
   "metadata": {},
   "source": [
    "## Prediction & confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa03a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the test data\n",
    "predicted = model_fit.predict(X_test)\n",
    "\n",
    "# Plot non-normalised confusion matrix\n",
    "plot_confusion_matrix(Y_test, predicted, classes=[\"No Attrition\", \"Yes Attrition\"])\n",
    "\n",
    "# Plot normalised confusion matrix\n",
    "plot_confusion_matrix(Y_test, predicted, classes=[\"No Attrition\", \"Yes Attrition\"], normalise=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f9a1ad",
   "metadata": {},
   "source": [
    "# Improvments to the model\n",
    "- Try different ensemble learning algorithms\n",
    "    - Random forests, XGBoost\n",
    "- More data which is balanced\n",
    "- More feature engineering\n",
    "- Further refining the hyper-parameters\n",
    "\n",
    "\n",
    "# Features to improve model\n",
    "- Gender\n",
    "- Job role\n",
    "- Overtime information\n",
    "- Last salary hike %\n",
    "- Total time working at the company\n",
    "- Time in current role\n",
    "- Time since last promotion\n",
    "- Time since last training provided by company"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d765c62c",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a73718",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# copying the dataframe to new variable\n",
    "scaled_df_xg=scaled_df\n",
    "target_xg=model_target\n",
    "\n",
    "\n",
    "\n",
    "# data_dmatrix = xgb.DMatrix(data=scaled_df_xg,label=target_xg)\n",
    "\n",
    "\n",
    "# Create train-test split\n",
    "X_train_xg, X_test_xg, Y_train_xg, Y_test_xg = train_test_split(scaled_df_xg, target_xg, test_size = 0.2, random_state=31081996)\n",
    "\n",
    "\n",
    "# Oversampling using SMOTE\n",
    "X_resampled_xg, Y_resampled_xg = SMOTE().fit_resample(X_train_xg, Y_train_xg)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bd9f2c",
   "metadata": {},
   "source": [
    "## Checking hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1fe999",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Tuning hyperparameters for XGboost\n",
    "X, y = X_resampled_xg, Y_resampled_xg\n",
    "\n",
    "\n",
    "\n",
    "params = { 'max_depth': [15],\n",
    "           'learning_rate': [0.01],\n",
    "          \"gamma\": [0.1],\n",
    "          \"reg_lambda\": [2],\n",
    "           'n_estimators': [800],\n",
    "          \"scale_pos_weight\": [12],\n",
    "           'colsample_bytree': [0.5]}\n",
    "scoring='roc_auc'\n",
    "\n",
    "xgb_cl = xgb.XGBClassifier(eval_metric='logloss',use_label_encoder=False,)\n",
    "clf = GridSearchCV( estimator=xgb_cl, \n",
    "                   param_grid=params,\n",
    "                   scoring=scoring, \n",
    "                   verbose=1)\n",
    "clf.fit(X, y)\n",
    "\n",
    "print(\"Best parameters:\", clf.best_params_)\n",
    "# Fit\n",
    "_ = xgb_cl.fit(X, y)\n",
    "\n",
    "# xgb_cl.best_score_\n",
    "\n",
    "# Best parameters: {'colsample_bytree': 0.7, 'gamma': 0, 'learning_rate': 0.01, 'max_depth': 10, 'n_estimators': 1000, 'reg_lambda': 1, 'scale_pos_weight': 5}\n",
    "#Applying tuned hyperparameter\n",
    "\n",
    "# Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
    "# Best parameters: {'colsample_bytree': 0.6, 'gamma': 0.1, 'learning_rate': 0.01, 'max_depth': 13, 'n_estimators': 900, 'reg_lambda': 2, 'scale_pos_weight': 6}\n",
    "\n",
    "# Fitting 5 folds for each of 144 candidates, totalling 720 fits\n",
    "# Best parameters: {'colsample_bytree': 0.5, 'gamma': 0.1, 'learning_rate': 0.01, 'max_depth': 13, 'n_estimators': 900, 'reg_lambda': 2, 'scale_pos_weight': 10}\n",
    "\n",
    "# Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
    "# Best parameters: {'colsample_bytree': 0.5, 'gamma': 0.1, 'learning_rate': 0.01, 'max_depth': 15, 'n_estimators': 900, 'reg_lambda': 2, 'scale_pos_weight': 12}\n",
    "\n",
    "# Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
    "# Best parameters: {'colsample_bytree': 0.5, 'gamma': 0.1, 'learning_rate': 0.01, 'max_depth': 15, 'n_estimators': 900, 'reg_lambda': 2, 'scale_pos_weight': 12}\n",
    "\n",
    "# Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
    "# Best parameters: {'colsample_bytree': 0.5, 'gamma': 0.1, 'learning_rate': 0.01, 'max_depth': 15, 'n_estimators': 800, 'reg_lambda': 2, 'scale_pos_weight': 12}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5451ae56",
   "metadata": {},
   "source": [
    "## Prediction & confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1598dc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fitting 5 folds for each of 2187 candidates, totalling 10935 fits\n",
    "# Best parameters: {'colsample_bytree': 0.7, 'gamma': 0, 'learning_rate': 0.01, 'max_depth': 10, 'n_estimators': 1000, 'reg_lambda': 1, 'scale_pos_weight': 5}\n",
    "#Applying tuned hyperparameter\n",
    "\n",
    "# eval_metric can have multiple values : logloss, error, binary:logistic\n",
    "\n",
    "xg_classification = xgb.XGBClassifier(eval_metric='logloss',use_label_encoder=False,colsample_bytree = 0.5, learning_rate = 0.01,\n",
    "                 max_depth = 15, n_estimators = 800,reg_lambda= 2,scale_pos_weight= 12)\n",
    "\n",
    "# https://xgboost.readthedocs.io/en/stable/parameter.html\n",
    "\n",
    "xg_classification.fit(X_resampled_xg, Y_resampled_xg)\n",
    "# xg_reg.fit(X_train_xg, Y_train_xg)\n",
    "\n",
    "\n",
    "# #Predicting value of test data\n",
    "\n",
    "Y_prediction_xg = xg_classification.predict(X_test_xg)\n",
    "Y_prediction_xg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f'XGBoost: {classification_report(Y_test_xg,Y_prediction_xg)}')\n",
    "# print(f'x resampled: {X_resampled.shape}')\n",
    "# print() \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Plot non-normalised confusion matrix\n",
    "# plot_confusion_matrix(Y_test_xg, Y_prediction_xg, classes=[\"No Attrition\", \"Yes Attrition\"])\n",
    "\n",
    "# Plot normalised confusion matrix\n",
    "plot_confusion_matrix(Y_test_xg, Y_prediction_xg, classes=[\"No Attrition\", \"Yes Attrition\"], normalise=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0f2a7f",
   "metadata": {},
   "source": [
    "## Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f37796",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_tree(xg_classification,num_trees=0)\n",
    "plt.rcParams['figure.figsize'] = [800, 800]\n",
    "plt.show()\n",
    "# plt.savefig('tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1156dc15",
   "metadata": {},
   "source": [
    "## Feature importance table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d564bd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb.plot_importance(xg_classification)\n",
    "# plt.rcParams['figure.figsize'] = [10, 10]\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd8abac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "map",
   "language": "python",
   "name": "map"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
